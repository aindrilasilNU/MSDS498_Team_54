{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code1: Scraping IMDB and Rotten Tomatoes for Netflix Movies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, sys, time, json, random, operator, itertools, pickle, csv, math \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('netflix_movies.csv')\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for t in list(movies_df['Title']):\n",
    "    titles.append(t.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RT search URLs\n",
    "search_urls = []\n",
    "for i in titles:\n",
    "    for j in i:\n",
    "        if j in ['#']:\n",
    "            i = i.replace(j, \"\")\n",
    "    search_urls.append('https://www.rottentomatoes.com/search?search=' + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape RT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT_scores_lst = []\n",
    "# reviews_cnt_lst = []\n",
    "# urls = []\n",
    "# not_on_RT = []\n",
    "\n",
    "# for i in search_urls:\n",
    "    \n",
    "#     if i=='https://www.rottentomatoes.com/search?search=7 aÃ±os':\n",
    "#         url='https://www.rottentomatoes.com/m/7_anos'\n",
    "#         urls.append(url)\n",
    "#         RT_scores_lst.append('')\n",
    "#         reviews_cnt_lst.append(2)\n",
    "#         continue\n",
    "\n",
    "#     if i=='https://www.rottentomatoes.com/search?search=One of Us':\n",
    "#         url='https://www.rottentomatoes.com/m/one_of_us_2017'\n",
    "#         urls.append(url)\n",
    "#         RT_scores_lst.append('')\n",
    "#         reviews_cnt_lst.append(2)\n",
    "#         continue \n",
    "        \n",
    "#     starting_page = i\n",
    "#     page = requests.get(starting_page)\n",
    "#     soup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "#     results = soup.find_all('search-page-result', slot='movie')\n",
    "    \n",
    "#     for r in results:\n",
    "#         row = r.find_all('search-page-media-row')\n",
    "#         for item in row:\n",
    "            \n",
    "#             RT_score = ''\n",
    "#             reviews_cnt = ''\n",
    "            \n",
    "#             url = item.find('a').get('href')\n",
    "#             page = requests.get(url)\n",
    "#             soup = BeautifulSoup(page.text, 'lxml')\n",
    "            \n",
    "#             w2w = soup.find_all('section', {'id':'where-to-watch'})\n",
    "#             if 'netflix' not in str(w2w).lower():\n",
    "#                 if item==row[-1]:\n",
    "#                     RT_scores_lst.append('DNE')\n",
    "#                     reviews_cnt_lst.append('DNE')\n",
    "#                     urls.append(i)\n",
    "#                     not_on_RT.append(i)\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     continue\n",
    "                \n",
    "#             topSection = soup.find_all('div', {'id':'topSection'})\n",
    "#             for j in topSection:\n",
    "#                 RT_score = j.find('score-board').get('tomatometerscore')\n",
    "#                 k = j.find_all('a')\n",
    "#                 for l in k:\n",
    "#                     if 'critics-count' in str(l):\n",
    "#                         reviews_cnt = str(l).split('\"critics-count\">')[1]\n",
    "#                         reviews_cnt = reviews_cnt.split(' ')[0]\n",
    "#                         reviews_cnt = int(reviews_cnt)\n",
    "#             RT_scores_lst.append(RT_score)\n",
    "#             reviews_cnt_lst.append(reviews_cnt)\n",
    "#             urls.append(url)\n",
    "            \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.append('https://www.rottentomatoes.com/m/oloture')\n",
    "RT_scores_lst.append('')\n",
    "reviews_cnt_lst.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_scores_lst2=[]\n",
    "reviews_cnt_lst2=[]\n",
    "for i,j in zip(RT_scores_lst,reviews_cnt_lst):\n",
    "    if i=='':\n",
    "        i=np.nan\n",
    "    if str(i).isalpha()==False:\n",
    "        i=int(i)\n",
    "    if str(j).isalpha()==False:\n",
    "        j=int(j)\n",
    "    RT_scores_lst2.append(i)\n",
    "    reviews_cnt_lst2.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_scores_df = pd.DataFrame(titles, columns=['Title'])\n",
    "movie_scores_df['Lang']=movies_df['Language']\n",
    "movie_scores_df['RT_score']=RT_scores_lst2\n",
    "movie_scores_df['RT_review_cnt']=reviews_cnt_lst2\n",
    "movie_scores_df['URL']=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix Enter the Anime results\n",
    "ind = list(movie_scores_df['Title']).index('Enter the Anime')\n",
    "movie_scores_df['RT_score'].loc[ind]=0\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=6\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/enter_the_anime'\n",
    "\n",
    "# fix ANIMA results\n",
    "ind = list(movie_scores_df['Title']).index('ANIMA')\n",
    "movie_scores_df['RT_score'].loc[ind]=100\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=22\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/anima_2020'\n",
    "\n",
    "# fix The Christmas Chronicles\n",
    "ind = list(movie_scores_df['Title']).index('The Christmas Chronicles')\n",
    "movie_scores_df['RT_score'].loc[ind]=68\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=59\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/the_christmas_chronicles'\n",
    "\n",
    "# fix The Babysitter\n",
    "ind = list(movie_scores_df['Title']).index('The Babysitter')\n",
    "movie_scores_df['RT_score'].loc[ind]=72\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=29\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/the_babysitter_2017'\n",
    "\n",
    "# fix The Princess Switch\n",
    "ind = list(movie_scores_df['Title']).index('The Princess Switch')\n",
    "movie_scores_df['RT_score'].loc[ind]=67\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=15\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/the_princess_switch'\n",
    "\n",
    "# fix Tall Girl\n",
    "ind = list(movie_scores_df['Title']).index('Tall Girl')\n",
    "movie_scores_df['RT_score'].loc[ind]=38\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=13\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/tall_girl'\n",
    "\n",
    "# fix Sweet & Sour\n",
    "ind = list(movie_scores_df['Title']).index('Sweet & Sour')\n",
    "movie_scores_df['RT_score'].loc[ind]=20\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=5\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/sweet_and_sour_2021'\n",
    "\n",
    "# fix I am Jonas\n",
    "ind = list(movie_scores_df['Title']).index('I am Jonas')\n",
    "movie_scores_df['RT_score'].loc[ind]=100\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=6\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/i_am_jonas'\n",
    "\n",
    "# fix A Christmas Prince\n",
    "ind = list(movie_scores_df['Title']).index('A Christmas Prince')\n",
    "movie_scores_df['RT_score'].loc[ind]=73\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=11\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/a_christmas_prince'\n",
    "\n",
    "# fix The Kissing Booth\n",
    "ind = list(movie_scores_df['Title']).index('The Kissing Booth')\n",
    "movie_scores_df['RT_score'].loc[ind]=15\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=13\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/the_kissing_booth'\n",
    "\n",
    "# fix The Silence of the Marsh\n",
    "ind = list(movie_scores_df['Title']).index('The Silence of the Marsh')\n",
    "movie_scores_df['RT_score'].loc[ind]=50\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=6\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/el_silencio_del_pantano'\n",
    "\n",
    "# fix One of Us\n",
    "ind = list(movie_scores_df['Title']).index('One of Us')\n",
    "movie_scores_df['RT_score'].loc[ind]=93\n",
    "movie_scores_df['RT_review_cnt'].loc[ind]=29\n",
    "movie_scores_df['URL'].loc[ind]='https://www.rottentomatoes.com/m/one_of_us_2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_scores_df = movie_scores_df.drop(movie_scores_df[movie_scores_df['RT_score']=='DNE'].index)\n",
    "\n",
    "drop_lst = [45,130,154,186,224,275,276,362,511,680,740,753,805,892,915,1019,1026,1049,1050,1093,1112,840,196,124,395,150,848,966,666]\n",
    "movie_scores_df = movie_scores_df.drop(drop_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in drop_lst:\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_comb_score = (movie_scores_df['RT_score']**2) * movie_scores_df['RT_review_cnt']\n",
    "\n",
    "RT_comb_score_norm = []\n",
    "for i in RT_comb_score:\n",
    "    i_norm = (i-RT_comb_score.min())/(RT_comb_score.max()-RT_comb_score.min())\n",
    "    RT_comb_score_norm.append(i_norm)\n",
    "\n",
    "movie_scores_df.insert(loc=4, column='RT_comb_score', value=RT_comb_score_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to .csv\n",
    "movie_scores_df.to_csv('RT_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "r = random.randint(0,len(movie_scores_df.index))\n",
    "movie_scores_df[['Title','URL']][r:r+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_scores_df[movie_scores_df['RT_review_cnt']>=5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movie_scores_df.sort_values(by='RT_comb_score', ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_scores_sub_df = movie_scores_df[movie_scores_df['RT_review_cnt']>=5]\n",
    "len(movie_scores_sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape IMDb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_titles = ['#Alive','#realityhigh','Cam','Sergio','End Game','Evelyn','Kate','Joy','Audrie & Daisy','Coffee & Kareem',\\\n",
    "'Deidra & Laney Rob a Train','Drive','Lionheart','Malcolm & Marie','Maska','Monster','Outlaws','Paradox','Ravenous',\\\n",
    "'Sweet & Sour','The Beast','The Highwaymen','The Main Event','The Silent War','The Square','Tig','Firedrake the Silver Dragon',\\\n",
    "'Flavors of Youth: International Version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_urls = ['https://www.imdb.com/title/tt10620868/','https://www.imdb.com/title/tt6119504/','https://www.imdb.com/title/tt8361028/',\\\n",
    "'https://www.imdb.com/title/tt8750570/','https://www.imdb.com/title/tt7879350/','https://www.imdb.com/title/tt8793990/',\\\n",
    "'https://www.imdb.com/title/tt7737528/','https://www.imdb.com/title/tt8917752/','https://www.imdb.com/title/tt5278460/',\\\n",
    "'https://www.imdb.com/title/tt9898858/','https://www.imdb.com/title/tt4144332/',\\\n",
    "'https://www.imdb.com/title/tt6593054/','https://www.imdb.com/title/tt7707314/','https://www.imdb.com/title/tt12676326/',\\\n",
    "'https://www.imdb.com/title/tt11953628/','https://www.imdb.com/title/tt2850272/','https://www.imdb.com/title/tt11892272/',\\\n",
    "'https://www.imdb.com/title/tt7949046/','https://www.imdb.com/title/tt6243140/','https://www.imdb.com/title/tt14599938/',\\\n",
    "'https://www.imdb.com/title/tt11499506/','https://www.imdb.com/title/tt1860242/','https://www.imdb.com/title/tt10540242/',\\\n",
    "'https://www.imdb.com/title/tt7336684/','https://www.imdb.com/title/tt2486682/','https://www.imdb.com/title/tt3986532/',\\\n",
    "'https://www.imdb.com/title/tt7080422/','https://www.imdb.com/title/tt8176578/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IMDb urls\n",
    "imdb_url_lst = []\n",
    "flag = []\n",
    "\n",
    "for t in movie_scores_sub_df['Title']:\n",
    "    \n",
    "    imdb_url=''\n",
    "    \n",
    "    for i in bad_titles:\n",
    "        if t==i:\n",
    "            imdb_url = bad_urls[bad_titles.index(i)]\n",
    "            imdb_url_lst.append(imdb_url)\n",
    "            break\n",
    "    \n",
    "    if t==i:\n",
    "        continue\n",
    "        \n",
    "    starting_page = 'https://www.imdb.com/find?q=' + t + '&ref_=nv_sr_sm'\n",
    "    page = requests.get(starting_page)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    main = soup.find('div',id='main')\n",
    "\n",
    "    results = main.find_all('tr')\n",
    "    \n",
    "    if results==[]:\n",
    "        imdb_url_lst.append('DNE')\n",
    "        flag.append(t)\n",
    "        continue\n",
    "\n",
    "    for r in results:\n",
    "\n",
    "        hyperlink = r.find('a')\n",
    "        suffix = str(hyperlink.get('href'))\n",
    "\n",
    "        if 'title' in suffix:\n",
    "\n",
    "            # verify correct movie\n",
    "            comp_credits = 'https://www.imdb.com'+suffix+'companycredits?ref_=tt_dt_co'\n",
    "            verify_page = requests.get(comp_credits)\n",
    "            soup = BeautifulSoup(verify_page.text, 'lxml')\n",
    "            main = soup.find('div',id='company_credits_content')\n",
    "            dist_comp=main.find_all('li')\n",
    "\n",
    "            # if correct movie, append to urls list\n",
    "            if 'Netflix' in str(dist_comp):\n",
    "                imdb_url = 'http://www.imdb.com' + suffix\n",
    "                imdb_url_lst.append(imdb_url)\n",
    "                break\n",
    "                \n",
    "            # if not correct movie, move on to next result\n",
    "            else:\n",
    "                if r==results[-1]:\n",
    "                    imdb_url_lst.append('DNE')\n",
    "                    flag.append(t)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        # if none of titles are Netflix movie, flag movie title and move on to next title\n",
    "        else:\n",
    "            imdb_url_lst.append('DNE')\n",
    "            flag.append(t)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.DataFrame(imdb_url_lst, columns=['URL'])\n",
    "# df['Title']=list(movie_scores_sub_df['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt+=1\n",
    "# r = random.randint(0,len(imdb_url_lst))\n",
    "# imdb_url_lst[r]\n",
    "# movie_scores_sub_df.iloc[r]\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix Perdida\n",
    "ind = list(movie_scores_sub_df['Title']).index('Perdida')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt7841496/'\n",
    "\n",
    "# fix Bleach\n",
    "ind = list(movie_scores_sub_df['Title']).index('Bleach')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt5979872/'\n",
    "\n",
    "# fix One of Us\n",
    "ind = list(movie_scores_sub_df['Title']).index('One of Us')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt7214842/'\n",
    "\n",
    "# fix Death Note\n",
    "ind = list(movie_scores_sub_df['Title']).index('Death Note')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt1241317/'\n",
    "\n",
    "# fix FullMetal Alchemist\n",
    "ind = list(movie_scores_sub_df['Title']).index('FullMetal Alchemist')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt5607028/'\n",
    "\n",
    "# fix Eye For An Eye\n",
    "ind = list(movie_scores_sub_df['Title']).index('Eye For An Eye')\n",
    "imdb_url_lst[ind]='https://www.imdb.com/title/tt7967412/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IMDb data (minus cast/crew)\n",
    "rel_year_lst = []\n",
    "genre_lst = []\n",
    "ratings_lst = []\n",
    "flag = []\n",
    "\n",
    "for url in imdb_url_lst: #\n",
    "    \n",
    "    rel_year=''\n",
    "    genre=''\n",
    "    rating=''\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    program_details = soup.find_all('a',{'class':'ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link'})\n",
    "\n",
    "    # get release info\n",
    "    for row in program_details:\n",
    "        row = str(row)\n",
    "        if 'releaseinfo' in row:\n",
    "            rel_info = row.split('releaseinfo')[1]\n",
    "            rel_year = re.search(r\"(\\d{4})\", str(rel_info))\n",
    "            rel_year = rel_year.group(1)\n",
    "            rel_year_lst.append(rel_year)\n",
    "            \n",
    "    plot_details = soup.find_all('ul',{'class':'ipc-metadata-list ipc-metadata-list--dividers-all sc-1d9a673d-1 cGogND ipc-metadata-list--base'})\n",
    "\n",
    "    for row in plot_details:\n",
    "        row2 = row.find_all('li',{'class':'ipc-inline-list__item'})\n",
    "\n",
    "        # get genre\n",
    "        genres = []\n",
    "        row_str = str(row2)\n",
    "\n",
    "        for s in re.finditer('genres=', row_str):\n",
    "            genre = row_str[s.start()+7:s.end()+30]\n",
    "            genre = genre.split('&amp')[0]\n",
    "            genres.append(genre)\n",
    "        genre_lst.append(genres)\n",
    "\n",
    "        # get rating\n",
    "        ratings = []\n",
    "        if 'Rated ' in row_str:\n",
    "            s = row_str.find('Rated ')\n",
    "            rating = row_str[s+6:s+15]\n",
    "            rating = rating.split(' for')[0]\n",
    "        if '\">TV-' in row_str:\n",
    "            s = row_str.find('\">TV-')\n",
    "            rating = row_str[s+2:s+10]\n",
    "            rating = rating.split('</')[0]\n",
    "    ratings_lst.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_year_lst_int = []\n",
    "for i in rel_year_lst:\n",
    "    i=int(i)\n",
    "    rel_year_lst_int.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list(movie_scores_sub_df['Title']), columns=['Title'])\n",
    "df['Rel_year']=rel_year_lst_int\n",
    "df['Genre']=genre_lst\n",
    "df['Rating']=ratings_lst\n",
    "df['URL']=imdb_url_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[6:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = df[df['Rating']=='']\n",
    "# abc[abc['Genre'].apply(lambda x: 'Drama' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('genre_rating_yr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('genre_rating_yr.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IMDb Cast Info/Filmographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crew(string, start, stop):\n",
    "    crew_str = str(string).split(start)[1]\n",
    "    crew_str = crew_str.split(stop)[0]\n",
    "    crew_str.find('<a href=\"')\n",
    "    \n",
    "    names=[]\n",
    "    links=[]\n",
    "\n",
    "    for s in re.finditer('<a href=\"', crew_str):\n",
    "        crew_link_0 = crew_str[s.end():s.end()+50]\n",
    "        crew_link = crew_link_0.split('\"> ')[0]\n",
    "        links.append(crew_link)\n",
    "        crew_name = crew_link_0.split('\"> ')[1]\n",
    "        crew_name = crew_name.split('\\n')[0]\n",
    "        names.append(crew_name)\n",
    "        \n",
    "    names = list(dict.fromkeys(names))\n",
    "    links = list(dict.fromkeys(links))\n",
    "    \n",
    "    return names, links\n",
    "\n",
    "def get_yr(string):\n",
    "\n",
    "    if ('in_production' not in str(string).lower()) and ('announced' not in str(string).lower()) and ('music video' not in str(string).lower()) and ('post-production' not in str(string).lower()) and ('pre-production' not in str(string).lower()):\n",
    "        if '(' in str(string):\n",
    "            paren = str(string).split('(')[1]\n",
    "            paren = paren.split(')')[0]\n",
    "\n",
    "            if not(any(item in paren.lower() for item in non_film_str)):\n",
    "                yr = str(string).split('\"year_column\">\\n\\xa0')[1]\n",
    "                yr = yr.split('\\n')[0]\n",
    "                return yr\n",
    "            else:\n",
    "                yr = ''\n",
    "                return yr\n",
    "        else:\n",
    "\n",
    "            yr = str(string).split('\"year_column\">\\n\\xa0')[1]\n",
    "            yr = yr.split('\\n')[0]\n",
    "            return yr\n",
    "    else:\n",
    "        yr = ''\n",
    "        return yr\n",
    "\n",
    "def get_title(string):\n",
    "\n",
    "    if ('in_production' not in str(string).lower()) and ('announced' not in str(string).lower()) and ('music video' not in str(string).lower()) and ('post-production' not in str(string).lower()) and ('pre-production' not in str(string).lower()):\n",
    "        if '(' in str(string):\n",
    "            paren = str(string).split('(')[1]\n",
    "            paren = paren.split(')')[0]\n",
    "\n",
    "            if not(any(item in paren.lower() for item in non_film_str)):\n",
    "                title = str(string.find('a')).split('/\">')[1]\n",
    "                title = title.split('</a>')[0]\n",
    "                return title\n",
    "            else:\n",
    "                title = ''\n",
    "                return title\n",
    "\n",
    "        else:\n",
    "            title = str(string.find('a')).split('/\">')[1]\n",
    "            title = title.split('</a>')[0]\n",
    "            return title\n",
    "    else:\n",
    "        title = ''\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_film_str = ['short','music video','segment','tv','video','in_production','announced','podcast',\\\n",
    "                'post-production','pre-production']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = 72\n",
    "\n",
    "dir_names_lst = []\n",
    "wrtr_names_lst = []\n",
    "actr_names_lst = []\n",
    "\n",
    "dir_projs_lst_final = []\n",
    "wrtr_projs_lst_final = []\n",
    "actr_projs_lst_final = []\n",
    "\n",
    "year_lst = []\n",
    "\n",
    "for link in list(df['URL'])[72:]:\n",
    "\n",
    "    page = requests.get(link) # + 'fullcredits?ref_=tt_ov_st_sm'\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    # get year\n",
    "    year_bar = soup.find_all('ul',{'class':'ipc-inline-list ipc-inline-list--show-dividers sc-8c396aa2-0 kqWovI baseAlt'})\n",
    "    year_str = str(year_bar).split('releaseinfo')[1]\n",
    "    year_str = year_str.split('\">')[1]\n",
    "    year_str = year_str.split('</a>')[0]\n",
    "    year = re.sub(\"[^0-9]\", \"\", year_str)\n",
    "    year = int(year)\n",
    "    year_lst.append(year)\n",
    "    \n",
    "    dir_names=[]\n",
    "    dir_links=[]\n",
    "\n",
    "    wrtr_names=[]\n",
    "    wrtr_links=[]\n",
    "\n",
    "    top_bar = soup.find_all('li',{'data-testid':'title-pc-principal-credit'})\n",
    "    for row in top_bar:\n",
    "        if 'Director' in str(row):\n",
    "            for s in re.finditer('rel=\"\">', str(row)):\n",
    "                name = str(row)[s.start()+7:s.end()+30]\n",
    "                name = name.split('</a>')[0]\n",
    "                dir_names.append(name)\n",
    "\n",
    "            for s in re.finditer('href=\"', str(row)):\n",
    "                link = str(row)[s.start()+6:s.end()+30]\n",
    "                link = link.split('?ref_')[0]\n",
    "                if 'name' in link:\n",
    "                    dir_links.append(link)\n",
    "                else:\n",
    "                    continue\n",
    "            break\n",
    "            \n",
    "    dir_names_lst.append(dir_names)\n",
    "\n",
    "    for row in top_bar:\n",
    "        if 'Writer' in str(row):\n",
    "            for s in re.finditer('rel=\"\">', str(row)):\n",
    "                name = str(row)[s.start()+7:s.end()+30]\n",
    "                name = name.split('</a>')[0]\n",
    "                wrtr_names.append(name)\n",
    "\n",
    "            for s in re.finditer('href=\"', str(row)):\n",
    "                link = str(row)[s.start()+6:s.end()+30]\n",
    "                link = link.split('?ref_')[0]\n",
    "                if 'name' in link:\n",
    "                    wrtr_links.append(link)\n",
    "                else:\n",
    "                    continue\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    wrtr_names_lst.append(wrtr_names)\n",
    "    \n",
    "    # get actors\n",
    "    \n",
    "    actr_names = []\n",
    "    actor_links = []\n",
    "\n",
    "    cast = soup.find_all('a',{'data-testid':'title-cast-item__actor'})\n",
    "    cnt = 0\n",
    "    for row in cast:\n",
    "        actor_name = re.search('>(.*)<', str(row)).group(1)\n",
    "        actr_names.append(actor_name)\n",
    "        \n",
    "        actor_link = str(row).split('href=\"/')[1]\n",
    "        actor_link = actor_link.split('?ref_')[0]\n",
    "        actor_links.append(actor_link)\n",
    "  \n",
    "        cnt+=1\n",
    "        if cnt==5:\n",
    "            break # stop loop after scraping top 5 actors\n",
    "    \n",
    "    actr_names_lst.append(actr_names)\n",
    "    \n",
    "    dir_projs_lst = []\n",
    "    \n",
    "    # get director(s) filmography\n",
    "    for link2 in dir_links:\n",
    "        page = requests.get('https://www.imdb.com/' + link2)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        filmography = soup.find('div',{'id':'filmography'})\n",
    "        films = filmography.find_all('div',{'class':\"filmo-row\"})\n",
    "        \n",
    "        dir_projs = []\n",
    "        dir_proj_yrs = []\n",
    "        \n",
    "        for f in films:\n",
    "            f_str = str(f)\n",
    "            \n",
    "            if 'id=\"director' in f_str:\n",
    "                \n",
    "                # get project year\n",
    "                yr = get_yr(f_str) \n",
    "                if yr=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    yr = re.sub(\"[^0-9]\", \"\", yr)\n",
    "                    yr = int(yr)\n",
    "                    if (yr>=year) or ((year-20)>yr): # only pull films made w/in 20 years of current film\n",
    "                        continue\n",
    "                    else:\n",
    "                        dir_proj_yrs.append(yr)\n",
    "                \n",
    "                # get project title\n",
    "                title = get_title(f)\n",
    "                if title=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    title = title.replace('&amp;','&')\n",
    "                    dir_projs.append(title)\n",
    "                    \n",
    "        dir_projs_dict = dict(zip(dir_projs, dir_proj_yrs))        \n",
    "        dir_projs_lst.append(dir_projs_dict)    \n",
    "    dir_projs_lst_final.append(dir_projs_lst)\n",
    "    \n",
    "    \n",
    "    wrtr_projs_lst = []\n",
    "    \n",
    "    # get writer(s) filmography\n",
    "    for link2 in wrtr_links:\n",
    "        page = requests.get('https://www.imdb.com/' + link2)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        filmography = soup.find('div',{'id':'filmography'})\n",
    "        films = filmography.find_all('div',{'class':\"filmo-row\"})\n",
    "        \n",
    "        wrtr_projs = []\n",
    "        wrtr_proj_yrs = []\n",
    "        \n",
    "        for f in films:\n",
    "            f_str = str(f)\n",
    "            \n",
    "            if 'id=\"writer' in f_str:\n",
    "                \n",
    "                # get project year\n",
    "                yr = get_yr(f_str) \n",
    "                if yr=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    yr = re.sub(\"[^0-9]\", \"\", yr)\n",
    "                    yr = int(yr)\n",
    "                    if (yr>=year) or ((year-20)>yr): # only pull films made w/in 20 years of current film\n",
    "                        continue\n",
    "                    else:\n",
    "                        wrtr_proj_yrs.append(yr)\n",
    "                \n",
    "                # get project title\n",
    "                title = get_title(f)\n",
    "                if title=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    title = title.replace('&amp;','&')\n",
    "                    wrtr_projs.append(title)\n",
    "                    \n",
    "        wrtr_projs_dict = dict(zip(wrtr_projs, wrtr_proj_yrs))                \n",
    "        wrtr_projs_lst.append(wrtr_projs_dict)    \n",
    "    wrtr_projs_lst_final.append(wrtr_projs_lst)\n",
    "            \n",
    "    \n",
    "    actr_projs_lst = []\n",
    "    \n",
    "    # get actor(s) filmography\n",
    "    for link2 in actor_links:\n",
    "        page = requests.get('https://www.imdb.com/' + link2)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        filmography = soup.find('div',{'id':'filmography'})\n",
    "        films = filmography.find_all('div',{'class':\"filmo-row\"})\n",
    "        \n",
    "        actr_projs = []\n",
    "        actr_proj_yrs = []\n",
    "        \n",
    "        for f in films:\n",
    "            f_str = str(f)\n",
    "            \n",
    "            if ('id=\"actor' in f_str) or ('id=\"actress' in f_str):\n",
    "                \n",
    "                # get project year\n",
    "                yr = get_yr(f_str) \n",
    "                if yr=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    yr = re.sub(\"[^0-9]\", \"\", yr)\n",
    "                    yr = int(yr)\n",
    "                    if (yr>=year) or ((year-10)>yr): # only pull films made w/in 10 years of current film\n",
    "                        continue\n",
    "                    else:\n",
    "                        actr_proj_yrs.append(yr)\n",
    "                \n",
    "                # get project title\n",
    "                title = get_title(f)\n",
    "                if title=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    title = title.replace('&amp;','&')\n",
    "                    actr_projs.append(title)\n",
    "                    \n",
    "        actr_projs_dict = dict(zip(actr_projs, actr_proj_yrs))            \n",
    "        actr_projs_lst.append(actr_projs_dict)   \n",
    "    actr_projs_lst_final.append(actr_projs_lst)\n",
    "    \n",
    "    cntr+=1\n",
    "    if ((dir_names_lst=='') and (wrtr_names_lst=='')) or (actr_names_lst==''):\n",
    "        print(f'Row {cntr} completed. {round((((cntr-73)/(622-73))*100),0)}% done. Possible missing info.')\n",
    "    else:\n",
    "        print(f'Row {cntr} completed. {round((((cntr-73)/(622-73))*100),0)}% done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = requests.get('https://www.imdb.com/title/tt6433832/')\n",
    "# soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "\n",
    "# dir_names=[]\n",
    "# dir_links=[]\n",
    "\n",
    "# wrtr_names=[]\n",
    "# wrtr_links=[]\n",
    "\n",
    "\n",
    "# top_bar = soup.find_all('li',{'data-testid':'title-pc-principal-credit'})\n",
    "# for row in top_bar:\n",
    "#     if 'Director' in str(row):\n",
    "#         for s in re.finditer('rel=\"\">', str(row)):\n",
    "#             name = str(row)[s.start()+7:s.end()+30]\n",
    "#             name = name.split('</a>')[0]\n",
    "#             dir_names.append(name)\n",
    "            \n",
    "#         for s in re.finditer('href=\"', str(row)):\n",
    "#             link = str(row)[s.start()+6:s.end()+30]\n",
    "#             link = link.split('?ref_')[0]\n",
    "#             if 'name' in link:\n",
    "#                 dir_links.append(link)\n",
    "#             else:\n",
    "#                 continue\n",
    "#         break\n",
    "\n",
    "# for row in top_bar:\n",
    "#     if 'Writer' in str(row):\n",
    "#         for s in re.finditer('rel=\"\">', str(row)):\n",
    "#             name = str(row)[s.start()+7:s.end()+30]\n",
    "#             name = name.split('</a>')[0]\n",
    "#             wrtr_names.append(name)\n",
    "            \n",
    "#         for s in re.finditer('href=\"', str(row)):\n",
    "#             link = str(row)[s.start()+6:s.end()+30]\n",
    "#             link = link.split('?ref_')[0]\n",
    "#             if 'name' in link:\n",
    "#                 wrtr_links.append(link)\n",
    "#             else:\n",
    "#                 continue\n",
    "#         break\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "# wrtr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[['Title','Rel_year']][0:72]\n",
    "# df2['Yr'] = year_lst[0:72]\n",
    "# df2['Director(s)'] = dir_names_lst[0:72]\n",
    "# df2['Director(s)_recent_work'] = dir_projs_lst_final[0:72]\n",
    "# df2['Writer(s)'] = wrtr_names_lst[0:72]\n",
    "# df2['Writer(s)_recent_work'] = wrtr_projs_lst_final[0:72]\n",
    "# df2['Actor(s)'] = actr_names_lst[0:72]\n",
    "# df2['Actor(s)_recent_work'] = actr_projs_lst_final[0:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2_new[:72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_2 = df[['Title','Rel_year']][72:]\n",
    "# df2_2['Yr'] = year_lst\n",
    "# df2_2['Director(s)'] = dir_names_lst\n",
    "# df2_2['Director(s)_recent_work'] = dir_projs_lst_final\n",
    "# df2_2['Writer(s)'] = wrtr_names_lst\n",
    "# df2_2['Writer(s)_recent_work'] = wrtr_projs_lst_final\n",
    "# df2_2['Actor(s)'] = actr_names_lst\n",
    "# df2_2['Actor(s)_recent_work'] = actr_projs_lst_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df2, df2_2])\n",
    "# df2 = df2.drop('Rel_year', 1)\n",
    "# df2.shape\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.rename(columns={'Yr': 'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv('raw_filmographies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_lst = []\n",
    "yrs_lst = []\n",
    "actrs_lst = []\n",
    "\n",
    "for lst,i in zip(list(df2['Actor(s)_recent_work']),range(len(df2['Actor(s)']))):\n",
    "    for d,actr in zip(lst,list(df2['Actor(s)'])[i]):\n",
    "        movies = list(d.keys())\n",
    "        yrs = list(d.values())\n",
    "        for m,y in zip(movies,yrs):\n",
    "            movies_lst.append(m)\n",
    "            yrs_lst.append(y)\n",
    "            actrs_lst.append(actr)\n",
    "\n",
    "df_actrs = pd.DataFrame(movies_lst, columns=['Title'])\n",
    "df_actrs['Year']=yrs_lst\n",
    "df_actrs['Cast/Crew']=actrs_lst\n",
    "df_actrs['Role']=['Actor']*len(df_actrs)\n",
    "df_actrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_lst = []\n",
    "yrs_lst = []\n",
    "dirs_lst = []\n",
    "\n",
    "for lst,i in zip(list(df2['Director(s)_recent_work']),range(len(df2['Director(s)']))):\n",
    "    for d,drctr in zip(lst,list(df2['Director(s)'])[i]):\n",
    "        movies = list(d.keys())\n",
    "        yrs = list(d.values())\n",
    "        for m,y in zip(movies,yrs):\n",
    "            movies_lst.append(m)\n",
    "            yrs_lst.append(y)\n",
    "            dirs_lst.append(drctr)\n",
    "\n",
    "df_dirs = pd.DataFrame(movies_lst, columns=['Title'])\n",
    "df_dirs['Year']=yrs_lst\n",
    "df_dirs['Cast/Crew']=dirs_lst\n",
    "df_dirs['Role']=['Director']*len(df_dirs)\n",
    "df_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_lst = []\n",
    "yrs_lst = []\n",
    "wrtrs_lst = []\n",
    "\n",
    "for lst,i in zip(list(df2['Writer(s)_recent_work']),range(len(df2['Writer(s)']))):\n",
    "    for d,wrtr in zip(lst,list(df2['Writer(s)'])[i]):\n",
    "        movies = list(d.keys())\n",
    "        yrs = list(d.values())\n",
    "        for m,y in zip(movies,yrs):\n",
    "            movies_lst.append(m)\n",
    "            yrs_lst.append(y)\n",
    "            wrtrs_lst.append(wrtr)\n",
    "\n",
    "df_wrtrs = pd.DataFrame(movies_lst, columns=['Title'])\n",
    "df_wrtrs['Year']=yrs_lst\n",
    "df_wrtrs['Cast/Crew']=wrtrs_lst\n",
    "df_wrtrs['Role']=['Writer']*len(df_wrtrs)\n",
    "df_wrtrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand = random.randint(0,2000)\n",
    "# df_actrs.iloc[rand]\n",
    "# df_dirs.iloc[rand]\n",
    "# df_wrtrs.iloc[rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.concat([df_actrs,df_dirs,df_wrtrs])\n",
    "df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_comb.groupby(['Title','Year','Role'],as_index=False)['Cast/Crew'].agg(lambda x: list(dict.fromkeys(list(x))))\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[df_grouped.duplicated(subset=['Title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actr = df_grouped[df_grouped['Role']=='Actor']\n",
    "df_dir = df_grouped[df_grouped['Role']=='Director']\n",
    "df_wrtr = df_grouped[df_grouped['Role']=='Writer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "data_frames = [df_actr,df_dir,df_wrtr]\n",
    "\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Title','Year'],\n",
    "                                            how='outer'), data_frames).fillna('')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_merged.drop(['Role_x','Role_y','Role'],1)\n",
    "df_merged2 = df_merged2.rename(columns={'Cast/Crew_x': 'Cast', 'Cast/Crew_y': 'Director', 'Cast/Crew': 'Writer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get RT Scores for Filmographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RT search URLs\n",
    "search_urls2 = []\n",
    "for i in list(df_merged2['Title']):\n",
    "    for j in i:\n",
    "        if j in ['#']:\n",
    "            i = i.replace(j, \"\")\n",
    "    search_urls2.append('https://www.rottentomatoes.com/search?search=' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr=0\n",
    "\n",
    "RT_scores_lst2 = []\n",
    "reviews_cnt_lst2 = []\n",
    "urls2 = []\n",
    "not_on_RT2 = []\n",
    "on_RT2 = []\n",
    "year_lst = []\n",
    "\n",
    "for i,j,k,l,m in zip(search_urls2, df_merged2['Year'], df_merged2['Cast'], df_merged2['Director'], df_merged2['Writer']):\n",
    "        \n",
    "    starting_page = i\n",
    "    page = requests.get(starting_page)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    title = i.split('https://www.rottentomatoes.com/search?search=')[1]\n",
    "    \n",
    "    results = soup.find_all('search-page-result', slot='movie')\n",
    "    \n",
    "    RT_score=''\n",
    "    \n",
    "    searching=True\n",
    "    \n",
    "    for r in results:\n",
    "        \n",
    "        if RT_score!='':\n",
    "            break\n",
    "        \n",
    "        row = r.find_all('search-page-media-row')\n",
    "        \n",
    "        for item in row:\n",
    "            \n",
    "            rel_yr = ''\n",
    "            cast = ''\n",
    "            RT_score = ''\n",
    "            reviews_cnt = ''\n",
    "            \n",
    "            if 'releaseyear' in str(item):\n",
    "                rel_yr = str(item).split('releaseyear=\"')[1]\n",
    "                rel_yr = rel_yr.split('\"')[0]\n",
    "                \n",
    "                if rel_yr.isdecimal()==False:\n",
    "                    continue\n",
    "\n",
    "                if abs(j-int(rel_yr))<=1: # select movie if within 1 year\n",
    "\n",
    "                    url = item.find('a').get('href')\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.text, 'lxml')\n",
    "                    \n",
    "                    if soup.find('section', {'id':'movie-cast'})==None:\n",
    "                        if item==row[-1]:\n",
    "                            RT_score='DNE'\n",
    "                            reviews_cnt='DNE'\n",
    "                            RT_scores_lst2.append(RT_score)\n",
    "                            reviews_cnt_lst2.append(reviews_cnt)\n",
    "                            urls2.append(i)\n",
    "                            not_on_RT2.append(i)\n",
    "                            on_RT2.append(title)\n",
    "                            year_lst.append(j)\n",
    "                            searching=False\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    cast_info = soup.find('section', {'id':'movie-cast'})\n",
    "                    cast_rows = cast_info.find_all('div',{'class':'cast-item media inlineBlock'})\n",
    "                    more_cast_rows = cast_info.find_all('div',{'class':'cast-item media inlineBlock moreCasts hide'})\n",
    "                    \n",
    "                    cast_names=[]\n",
    "                    for row2 in cast_rows:\n",
    "                        if 'span title=\"' in str(row2):\n",
    "                            name = str(row2).split('span title=\"')[1]\n",
    "                            name = name.split('\">')[0]\n",
    "                            cast_names.append(name)\n",
    "                    for row2 in more_cast_rows:\n",
    "                        if 'span title=\"' in str(row2):\n",
    "                            name = str(row2).split('span title=\"')[1]\n",
    "                            name = name.split('\">')[0]\n",
    "                            cast_names.append(name)\n",
    "                    \n",
    "                    cast=False\n",
    "                    \n",
    "                    for a in k:\n",
    "                        if a in cast_names:\n",
    "                            cast=True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                    for d in l:\n",
    "                        if d in cast_names:\n",
    "                            cast=True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    for w in m:\n",
    "                        if w in cast_names:\n",
    "                            cast=True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    if cast==True:\n",
    "                        topSection = soup.find_all('div', {'id':'topSection'})\n",
    "                        for t in topSection:\n",
    "                            RT_score = t.find('score-board').get('tomatometerscore')\n",
    "                            f = t.find_all('a')\n",
    "                            for g in f:\n",
    "                                if 'critics-count' in str(g):\n",
    "                                    reviews_cnt = str(g).split('\"critics-count\">')[1]\n",
    "                                    reviews_cnt = reviews_cnt.split(' ')[0]\n",
    "                                    reviews_cnt = int(reviews_cnt)\n",
    "                        RT_scores_lst2.append(RT_score)\n",
    "                        reviews_cnt_lst2.append(reviews_cnt)\n",
    "                        urls2.append(url)\n",
    "                        on_RT2.append(title)\n",
    "                        year_lst.append(j)\n",
    "                        searching=False\n",
    "                        break \n",
    "                        \n",
    "                    elif j==int(rel_yr):\n",
    "                        topSection = soup.find_all('div', {'id':'topSection'})\n",
    "                        for t in topSection:\n",
    "                            f = t.find('h1',{'class':'scoreboard__title'})\n",
    "                            if 'slot=\"title\">' in str(f):\n",
    "                                RT_title = str(f).split('slot=\"title\">')[1]\n",
    "                                RT_title = RT_title.split('</h1>')[0]\n",
    "                                RT_title = RT_title.replace('&amp;','&')\n",
    "                                RT_title = re.sub(r'[^\\w\\s]', '', RT_title)\n",
    "                                no_punc_title = re.sub(r'[^\\w\\s]', '', title)\n",
    "                            if no_punc_title.lower()==RT_title.lower():\n",
    "                                for t in topSection:\n",
    "                                    RT_score = t.find('score-board').get('tomatometerscore')\n",
    "                                    f = t.find_all('a')\n",
    "                                    for g in f:\n",
    "                                        if 'critics-count' in str(g):\n",
    "                                            reviews_cnt = str(g).split('\"critics-count\">')[1]\n",
    "                                            reviews_cnt = reviews_cnt.split(' ')[0]\n",
    "                                            reviews_cnt = int(reviews_cnt)\n",
    "                                RT_scores_lst2.append(RT_score)\n",
    "                                reviews_cnt_lst2.append(reviews_cnt)\n",
    "                                urls2.append(url)\n",
    "                                on_RT2.append(title)\n",
    "                                year_lst.append(j)\n",
    "                                searching=False\n",
    "                                break\n",
    "                            else:\n",
    "                                if item==row[-1]:\n",
    "                                    RT_score='DNE'\n",
    "                                    reviews_cnt='DNE'\n",
    "                                    RT_scores_lst2.append(RT_score)\n",
    "                                    reviews_cnt_lst2.append(reviews_cnt)\n",
    "                                    urls2.append(i)\n",
    "                                    not_on_RT2.append(i)\n",
    "                                    on_RT2.append(title)\n",
    "                                    year_lst.append(j)\n",
    "                                    searching=False\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                                \n",
    "                        break\n",
    "                    \n",
    "                    else:\n",
    "                        if item==row[-1]:\n",
    "                            RT_score='DNE'\n",
    "                            reviews_cnt='DNE'\n",
    "                            RT_scores_lst2.append(RT_score)\n",
    "                            reviews_cnt_lst2.append(reviews_cnt)\n",
    "                            urls2.append(i)\n",
    "                            not_on_RT2.append(i)\n",
    "                            on_RT2.append(title)\n",
    "                            year_lst.append(j)\n",
    "                            searching=False\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                else:\n",
    "                    if item==row[-1]:\n",
    "                        RT_score='DNE'\n",
    "                        reviews_cnt='DNE'\n",
    "                        RT_scores_lst2.append(RT_score)\n",
    "                        reviews_cnt_lst2.append(reviews_cnt)\n",
    "                        urls2.append(i)\n",
    "                        not_on_RT2.append(i)\n",
    "                        on_RT2.append(title)\n",
    "                        year_lst.append(j)\n",
    "                        searching=False\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if searching==True:\n",
    "            RT_score='DNE'\n",
    "            reviews_cnt='DNE'\n",
    "            RT_scores_lst2.append(RT_score)\n",
    "            reviews_cnt_lst2.append(reviews_cnt)\n",
    "            urls2.append(i)\n",
    "            not_on_RT2.append(i)\n",
    "            on_RT2.append(title)\n",
    "            year_lst.append(j)\n",
    "            searching==False\n",
    "            break  \n",
    "\n",
    "    cntr+=1\n",
    "    if RT_score=='DNE':\n",
    "        print(f'Row {cntr} completed. {round(cntr/len(search_urls2)*100,0)}% done. {len(not_on_RT2)} movies not found.')\n",
    "    else:\n",
    "        print(f'Row {cntr} completed. {round(cntr/len(search_urls2)*100,0)}% done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(urls2,columns=['URL'])\n",
    "output_df['Title']=on_RT2\n",
    "output_df['Year']=year_lst\n",
    "output_df['Score']=RT_scores_lst2\n",
    "output_df['Revs']=reviews_cnt_lst2\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_222 = dict({'URL':\"https://www.rottentomatoes.com/m/222_2017\",'Title':'2:22','Year':2017,'Score':22,'Revs':9})\n",
    "# d_DA = dict({'URL':\"https://www.rottentomatoes.com/m/dance_academy_the_comeback\",'Title':'Dance Academy: The Movie','Year':2017,'Score':100,'Revs':15})\n",
    "# d_DE = dict({'URL':\"https://www.rottentomatoes.com/m/donovans_echo\",'Title':\"Donovan's Echo\",'Year':2011,'Score':33,'Revs':9})\n",
    "# d_He = dict({'URL':\"https://www.rottentomatoes.com/m/hes_just_not_that_into_you\",'Title':\"He's Just Not That Into You\",'Year':2009,'Score':41,'Revs':169})\n",
    "# d_NW = dict({'URL':\"https://www.rottentomatoes.com/m/nobody_walks\",'Title':'Nobody Walks','Year':2012,'Score':41,'Revs':41})\n",
    "# d_Nor1 = dict({'URL':\"https://www.rottentomatoes.com/m/norman\",'Title':'Norman','Year':2010,'Score':67,'Revs':15})\n",
    "# d_Nor2 = dict({'URL':\"https://www.rottentomatoes.com/m/norman_2017\",'Title':'Norman','Year':2016,'Score':87,'Revs':127})\n",
    "\n",
    "# output_df_fixed = output_df.append(d_222, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_DA, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_DE, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_He, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_NW, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_Nor1, ignore_index=True)\n",
    "# output_df_fixed = output_df_fixed.append(d_Nor2, ignore_index=True)\n",
    "output_df_fixed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmography_RT_data = output_df_fixed[(output_df_fixed['Score']!='') & (output_df_fixed['Score']!='DNE')]\n",
    "filmography_RT_data = filmography_RT_data.reset_index(drop=True)\n",
    "filmography_RT_data['Score'] = pd.to_numeric(filmography_RT_data['Score'])\n",
    "filmography_RT_data['Revs'] = pd.to_numeric(filmography_RT_data['Revs'])\n",
    "filmography_RT_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_comb_score2 = (filmography_RT_data['Score']**2) * filmography_RT_data['Revs']\n",
    "RT_comb_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_comb_score_norm2=[]\n",
    "for i in RT_comb_score2:\n",
    "    i_norm = (i-RT_comb_score2.min())/(RT_comb_score2.max()-RT_comb_score2.min())\n",
    "    RT_comb_score_norm2.append(i_norm)\n",
    "\n",
    "filmography_RT_data.insert(loc=5, column='RT_comb_score', value=RT_comb_score_norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmography_RT_data.sort_values(by='RT_comb_score',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filmography_RT_data.to_csv('filmography_RTscores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More IMDb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months=['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "mon = list(range(1,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = 0\n",
    "\n",
    "actr_names_lst = []\n",
    "actr_projs_lst_final = []\n",
    "\n",
    "rel_month_lst = []\n",
    "runtime_lst = []\n",
    "year_lst = []\n",
    "\n",
    "for link in list(df['URL'][467:]):\n",
    "\n",
    "    page = requests.get(link) # + 'fullcredits?ref_=tt_ov_st_sm'\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    # get release info (month)\n",
    "    program_details = soup.find_all('a',{'class':'ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link'})\n",
    "    for row in program_details:\n",
    "        row = str(row)\n",
    "        if 'releaseinfo' in row:\n",
    "            rel_info = row.split('releaseinfo')[1]\n",
    "            for m in months:\n",
    "                if m in str(rel_info):\n",
    "                    rel_month_lst.append(mon[months.index(m)])\n",
    "                    \n",
    "    # get runtime\n",
    "    runtime_str = soup.find('li',{'data-testid':'title-techspec_runtime'})\n",
    "    runtime_str = runtime_str.find('div',{'class':'ipc-metadata-list-item__content-container'})\n",
    "    if 'hour' in runtime_str:\n",
    "        hour = str(runtime_str).split('<!-- --> <!-- -->hour')[0]\n",
    "        hour = hour.split('container\">')[1]\n",
    "        hour = int(hour)\n",
    "        minutes = hour*60\n",
    "    elif 'hours' in runtime_str:\n",
    "        hour = str(runtime_str).split('<!-- --> <!-- -->hours')[0]\n",
    "        hour = hour.split('container\">')[1]\n",
    "        hour = int(hour)\n",
    "        minutes = hour*60\n",
    "    else:\n",
    "        minutes = 0\n",
    "    if 'minutes' in runtime_str:\n",
    "        min_ = str(runtime_str).split('<!-- --> <!-- -->minutes')[0][-4:]\n",
    "        min_ = min_.split('>')[1]\n",
    "        min_ = int(min_)\n",
    "        \n",
    "        minutes = minutes + min_\n",
    "        runtime_lst.append(minutes)\n",
    "    else:\n",
    "        runtime_lst.append(minutes)\n",
    "    \n",
    "    # get year\n",
    "    year_bar = soup.find_all('ul',{'class':'ipc-inline-list ipc-inline-list--show-dividers sc-8c396aa2-0 kqWovI baseAlt'})\n",
    "    year_str = str(year_bar).split('releaseinfo')[1]\n",
    "    year_str = year_str.split('\">')[1]\n",
    "    year_str = year_str.split('</a>')[0]\n",
    "    year = re.sub(\"[^0-9]\", \"\", year_str)\n",
    "    year = int(year)\n",
    "    year_lst.append(year)\n",
    "    \n",
    "    # get actors\n",
    "    \n",
    "    actr_names = []\n",
    "    actor_links = []\n",
    "\n",
    "    cast = soup.find_all('a',{'data-testid':'title-cast-item__actor'})\n",
    "    cnt = 0\n",
    "    for row in cast:\n",
    "        actor_name = re.search('>(.*)<', str(row)).group(1)\n",
    "        actr_names.append(actor_name)\n",
    "        \n",
    "        actor_link = str(row).split('href=\"/')[1]\n",
    "        actor_link = actor_link.split('?ref_')[0]\n",
    "        actor_links.append(actor_link)\n",
    "  \n",
    "        cnt+=1\n",
    "        if cnt==5:\n",
    "            break # stop loop after scraping top 5 actors\n",
    "    \n",
    "    actr_names_lst.append(actr_names)\n",
    "    \n",
    "    actr_projs_lst = []\n",
    "    \n",
    "    # get actor(s) filmography\n",
    "    for link2,actor2 in zip(actor_links,actr_names):\n",
    "        page = requests.get('https://www.imdb.com/' + link2)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        filmography = soup.find('div',{'id':'filmography'})\n",
    "        films = filmography.find_all('div',{'class':\"filmo-row\"})\n",
    "        \n",
    "        actr_projs = []\n",
    "        actr_proj_yrs = []\n",
    "        rank_lst = []\n",
    "        \n",
    "        for f in films:\n",
    "            f_str = str(f)\n",
    "\n",
    "            if ('id=\"actor' in f_str) or ('id=\"actress' in f_str):\n",
    "\n",
    "                # get project year\n",
    "                yr = get_yr(f_str) \n",
    "                if yr=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    yr = re.sub(\"[^0-9]\", \"\", yr)\n",
    "                    yr = int(yr)\n",
    "                    if (yr>=year) or ((year-10)>yr): # only pull films made w/in 10 years of current film\n",
    "                        continue\n",
    "                    else:\n",
    "                        actr_proj_yrs.append(yr)\n",
    "\n",
    "                # get project title\n",
    "                title = get_title(f)\n",
    "\n",
    "                if title=='':\n",
    "                    continue\n",
    "                else:\n",
    "                    title = title.replace('&amp;','&')\n",
    "                    actr_projs.append(title)\n",
    "\n",
    "                    title_URL = f.find('a').get('href') # get project URL\n",
    "\n",
    "                    page = requests.get('https://www.imdb.com/' + title_URL)\n",
    "                    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "                    actr_names2 = []\n",
    "                    \n",
    "                    cast = soup.find_all('a',{'data-testid':'title-cast-item__actor'})\n",
    "                    for cast_row in cast:\n",
    "                        name = re.search('>(.*)<', str(cast_row)).group(1)\n",
    "                        actr_names2.append(name)\n",
    "                    \n",
    "                    if actor2 in actr_names2:\n",
    "                        rank = actr_names2.index(actor2)+1\n",
    "                        rank_lst.append(rank) \n",
    "                    else:\n",
    "                        rank = np.nan\n",
    "                        rank_lst.append(rank)    \n",
    "        \n",
    "        actr_projs_dict = {}\n",
    "        for proj,proj_ind in zip(actr_projs,range(len(actr_projs))):\n",
    "            actr_projs_dict[proj] = [actr_proj_yrs[proj_ind], rank_lst[proj_ind]]         \n",
    "        actr_projs_lst.append(actr_projs_dict)   \n",
    "    actr_projs_lst_final.append(actr_projs_lst)\n",
    "    \n",
    "    cntr+=1\n",
    "    if ((dir_names_lst=='') and (wrtr_names_lst=='')) or (actr_names_lst==''):\n",
    "        print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done. Possible missing info.')\n",
    "    else:\n",
    "        print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filmographies_2 = df[['Title']][467:]\n",
    "df_filmographies_2['Rel_year'] = year_lst\n",
    "df_filmographies_2['Rel_month'] = rel_month_lst\n",
    "df_filmographies_2['Actor(s)'] = actr_names_lst\n",
    "df_filmographies_2['Actor(s)_recent_work'] = actr_projs_lst_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filmographies = pd.concat([df_filmographies_1, df_filmographies_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filmographies # add runtime, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filmographies.to_csv('raw_filmographies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = 0\n",
    "\n",
    "runtime_lst = []\n",
    "\n",
    "for link in list(df['URL']):\n",
    "\n",
    "    page = requests.get(link) # + 'fullcredits?ref_=tt_ov_st_sm'\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "                    \n",
    "    # get runtime\n",
    "    runtime_str = soup.find('li',{'data-testid':'title-techspec_runtime'})\n",
    "    runtime_str = runtime_str.find('div',{'class':'ipc-metadata-list-item__content-container'})\n",
    "    if 'hour' in runtime_str:\n",
    "        hour = str(runtime_str).split('<!-- --> <!-- -->hour')[0]\n",
    "        hour = hour.split('container\">')[1]\n",
    "        hour = int(hour)\n",
    "        minutes = hour*60\n",
    "    elif 'hours' in runtime_str:\n",
    "        hour = str(runtime_str).split('<!-- --> <!-- -->hours')[0]\n",
    "        hour = hour.split('container\">')[1]\n",
    "        hour = int(hour)\n",
    "        minutes = hour*60\n",
    "    else:\n",
    "        minutes = 0\n",
    "    if 'minutes' in runtime_str:\n",
    "        min_ = str(runtime_str).split('<!-- --> <!-- -->minutes')[0][-4:]\n",
    "        min_ = min_.split('>')[1]\n",
    "        min_ = int(min_)\n",
    "        \n",
    "        minutes = minutes + min_\n",
    "        runtime_lst.append(minutes)\n",
    "    elif 'minute' in runtime_str:\n",
    "        min_ = str(runtime_str).split('<!-- --> <!-- -->minute')[0][-4:]\n",
    "        min_ = min_.split('>')[1]\n",
    "        min_ = int(min_)\n",
    "        \n",
    "        minutes = minutes + min_\n",
    "        runtime_lst.append(minutes)\n",
    "    else:\n",
    "        runtime_lst.append(minutes)\n",
    "    \n",
    "    cntr+=1\n",
    "    if ((dir_names_lst=='') and (wrtr_names_lst=='')) or (actr_names_lst==''):\n",
    "        print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done. Possible missing info.')\n",
    "    else:\n",
    "        print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filmographies.head(2)\n",
    "len(df_filmographies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_scores_df = pd.read_csv('RT_scores.csv')\n",
    "movie_scores_df = movie_scores_df[movie_scores_df['RT_review_cnt']>=5].reset_index(drop=True)\n",
    "movie_scores_df.head(2)\n",
    "len(movie_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmography_RT_data.head(2)\n",
    "len(filmography_RT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_comb = movie_scores_df[['Title','Lang']]\n",
    "df_comb['Rel_year'] = df_filmographies['Rel_year']\n",
    "df_comb['Rel_month'] = df_filmographies['Rel_month']\n",
    "df_comb['Genre'] = df['Genre']\n",
    "df_comb['Rating'] = df['Rating']\n",
    "df_comb['Runtime'] = runtime_lst\n",
    "df_comb['Director'] = df2['Director(s)']\n",
    "df_comb['Dir_recent_work'] = df2['Director(s)_recent_work']\n",
    "df_comb['Writer'] = df2['Writer(s)']\n",
    "df_comb['Writer_recent_work'] = df2['Writer(s)_recent_work']\n",
    "df_comb['Cast'] = df_filmographies['Actor(s)']\n",
    "df_comb['Cast_recent_work'] = df_filmographies['Actor(s)_recent_work']\n",
    "df_comb['RT_score'] = movie_scores_df['RT_score']\n",
    "df_comb['RT_rev_cnt'] = movie_scores_df['RT_review_cnt']\n",
    "df_comb['RT_comb_score'] = movie_scores_df['RT_comb_score']\n",
    "df_comb['RT_url'] = movie_scores_df['URL']\n",
    "df_comb['IMDb_url'] = df['URL']\n",
    "\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comb.to_csv('df_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cast/Crew Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirs = pd.DataFrame(df_comb['Director'].to_list(), columns=['Dir1', 'Dir2', 'Dir3'])\n",
    "df_dirs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir_films = pd.DataFrame(df_comb['Dir_recent_work'].to_list(), columns=['Dir1_films', 'Dir2_films', 'Dir3_films'])\n",
    "df_dir_films.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer = pd.DataFrame(df_comb['Writer'].to_list(), columns=['Writer1', 'Writer2', 'Writer3'])\n",
    "df_writer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_films = pd.DataFrame(df_comb['Writer_recent_work'].to_list(), columns=['Writer1_films', 'Writer2_films', 'Writer3_films'])\n",
    "df_writer_films.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(data)):\n",
    "#     data['Actor(s)'].iloc[i] = literal_eval(data['Actor(s)'].iloc[i])\n",
    "\n",
    "df_actors = pd.DataFrame(df_comb['Cast'].to_list(), columns=['Actor1', 'Actor2', 'Actor3', 'Actor4', 'Actor5'])\n",
    "df_actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(data)):\n",
    "#     films = data['Actor(s)_recent_work'].iloc[i]\n",
    "#     films = films.replace('nan','0')\n",
    "#     data['Actor(s)_recent_work'].iloc[i] = literal_eval(films)\n",
    "\n",
    "df_actors_films = pd.DataFrame(df_comb['Cast_recent_work'].to_list(), columns=['Actor1_films', 'Actor2_films', 'Actor3_films','Actor4_films','Actor5_films'])\n",
    "df_actors_films.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate quality of crew\n",
    "def calc_crew_qual(cast_crew_col):\n",
    "    \n",
    "    cast_crew_qual = []\n",
    "    cast_crew_film_scores = []\n",
    "    \n",
    "    for i in range(len(df_comb)):\n",
    "        scores_ = []\n",
    "        dic = cast_crew_col.iloc[i]\n",
    "        if dic!=None:\n",
    "            for j in dic:\n",
    "                year = dic.get(j)\n",
    "                if ((filmography_RT_data['Title']==j) & (filmography_RT_data['Year']==year)).any():\n",
    "                    movie_score = filmography_RT_data[(filmography_RT_data['Title']==j) & (filmography_RT_data['Year']==year)]['RT_comb_score'].item()\n",
    "                    cast_crew_score = movie_score\n",
    "                else:\n",
    "                    movie_score = np.nan\n",
    "                    cast_crew_score = np.nan\n",
    "                scores_.append(cast_crew_score)\n",
    "            if all(x is np.nan for x in scores_):\n",
    "                cast_crew_score = np.nan\n",
    "            else:\n",
    "                cast_crew_score = np.nansum(scores_)/math.sqrt(len(scores_)) # np.count_nonzero(~np.isnan(scores_)) / len(scores_)\n",
    "            cast_crew_film_scores.append(scores_)\n",
    "            cast_crew_qual.append(cast_crew_score)\n",
    "        else:\n",
    "            cast_crew_film_scores.append(np.nan)\n",
    "            cast_crew_qual.append(np.nan)\n",
    "            \n",
    "    cast_crew_qual_lst = []\n",
    "    for i in cast_crew_qual:\n",
    "        if i is np.nan:\n",
    "            i_norm = np.nan\n",
    "        else:\n",
    "            i_norm = (i-np.nanmin(cast_crew_qual))/(np.nanmax(cast_crew_qual)-np.nanmin(cast_crew_qual))\n",
    "        cast_crew_qual_lst.append(i_norm)\n",
    "    \n",
    "    return cast_crew_film_scores, cast_crew_qual_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate quality of cast\n",
    "def calc_cast_qual(cast_crew_col):\n",
    "    \n",
    "    cast_crew_qual = []\n",
    "    cast_crew_film_scores = []\n",
    "    \n",
    "    for i in range(len(df_comb)):\n",
    "        scores_ = []\n",
    "        dic = cast_crew_col.iloc[i]\n",
    "        if dic!=None:\n",
    "            for j in dic:\n",
    "                year = dic.get(j)[0]\n",
    "                rank = dic.get(j)[1]\n",
    "                if rank==0:\n",
    "                    rank=np.nan\n",
    "                if ((filmography_RT_data['Title']==j) & (filmography_RT_data['Year']==year)).any():\n",
    "                    movie_score = filmography_RT_data[(filmography_RT_data['Title']==j) & (filmography_RT_data['Year']==year)]['RT_comb_score'].item()\n",
    "                    cast_crew_score = movie_score/math.sqrt(rank)\n",
    "                else:\n",
    "                    movie_score = np.nan\n",
    "                    cast_crew_score = np.nan\n",
    "                scores_.append(cast_crew_score)\n",
    "            if all(x is np.nan for x in scores_):\n",
    "                cast_crew_score = np.nan\n",
    "            else:\n",
    "                cast_crew_score = np.nansum(scores_)/math.sqrt(len(scores_)) # np.count_nonzero(~np.isnan(scores_)) / len(scores_)\n",
    "            cast_crew_film_scores.append(scores_)\n",
    "            cast_crew_qual.append(cast_crew_score)\n",
    "        else:\n",
    "            cast_crew_film_scores.append(np.nan)\n",
    "            cast_crew_qual.append(np.nan)\n",
    "            \n",
    "    cast_crew_qual_lst = []\n",
    "    for i in cast_crew_qual:\n",
    "        if i is np.nan:\n",
    "            i_norm = np.nan\n",
    "        else:\n",
    "            i_norm = (i-np.nanmin(cast_crew_qual))/(np.nanmax(cast_crew_qual)-np.nanmin(cast_crew_qual))\n",
    "        cast_crew_qual_lst.append(i_norm)\n",
    "    \n",
    "    return cast_crew_film_scores, cast_crew_qual_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1_film_scores, dir1_qual = calc_crew_qual(df_dir_films['Dir1_films'])\n",
    "dir2_film_scores, dir2_qual = calc_crew_qual(df_dir_films['Dir2_films'])\n",
    "dir3_film_scores, dir3_qual = calc_crew_qual(df_dir_films['Dir3_films'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir_final = df_comb[['Title','Rel_year','RT_rev_cnt','RT_comb_score']]\n",
    "df_dir_final['Dir1'] = df_dirs[['Dir1']]\n",
    "df_dir_final['Dir1_films'] = df_dir_films[['Dir1_films']]\n",
    "df_dir_final['Dir1_film_scores'] = dir1_film_scores\n",
    "df_dir_final['Dir1_qual']=dir1_qual\n",
    "\n",
    "df_dir_final['Dir2'] = df_dirs[['Dir2']]\n",
    "df_dir_final['Dir2_films'] = df_dir_films[['Dir2_films']]\n",
    "df_dir_final['Dir2_film_scores'] = dir2_film_scores\n",
    "df_dir_final['Dir2_qual']=dir2_qual\n",
    "\n",
    "df_dir_final['Dir3'] = df_dirs[['Dir3']]\n",
    "df_dir_final['Dir3_films'] = df_dir_films[['Dir3_films']]\n",
    "df_dir_final['Dir3_film_scores'] = dir3_film_scores\n",
    "df_dir_final['Dir3_qual']=dir3_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir_final['Dir_avg_qual'] = df_dir_final[['Dir1_qual','Dir2_qual','Dir3_qual']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dir_final2 = df_dir_final.sort_values(by='RT_comb_score', ascending=False)[['Title','RT_comb_score','Dir1','Dir1_films','Dir1_qual']]\n",
    "df_dir_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir_final2.plot.scatter(x='Title', y='Dir1_qual',c='DarkBlue', rot=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_dir_final2.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1_film_scores, writer1_qual = calc_crew_qual(df_writer_films['Writer1_films'])\n",
    "writer2_film_scores, writer2_qual = calc_crew_qual(df_writer_films['Writer2_films'])\n",
    "writer3_film_scores, writer3_qual = calc_crew_qual(df_writer_films['Writer3_films'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_final = df_comb[['Title','Rel_year','RT_rev_cnt','RT_comb_score']]\n",
    "df_writer_final['Writer1'] = df_writer[['Writer1']]\n",
    "df_writer_final['Writer1_films'] = df_writer_films[['Writer1_films']]\n",
    "df_writer_final['Writer1_film_scores'] = writer1_film_scores\n",
    "df_writer_final['Writer1_qual'] = writer1_qual\n",
    "\n",
    "df_writer_final['Writer2'] = df_writer[['Writer2']]\n",
    "df_writer_final['Writer2_films'] = df_writer_films[['Writer2_films']]\n",
    "df_writer_final['Writer2_film_scores'] = writer2_film_scores\n",
    "df_writer_final['Writer2_qual'] = writer2_qual\n",
    "\n",
    "df_writer_final['Writer3'] = df_writer[['Writer3']]\n",
    "df_writer_final['Writer3_films'] = df_writer_films[['Writer3_films']]\n",
    "df_writer_final['Writer3_film_scores'] = writer3_film_scores\n",
    "df_writer_final['Writer3_qual'] = writer3_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_final['Writer_avg_qual'] = df_writer_final[['Writer1_qual','Writer2_qual','Writer3_qual']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_final2 = df_writer_final.sort_values(by='RT_comb_score', ascending=False)[['Title','RT_comb_score','Writer1','Writer1_films','Writer1_qual']]\n",
    "df_writer_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_final2.plot.scatter(x='Title', y='Writer1_qual',c='DarkBlue', rot=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_writer_final2.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor1_film_scores, actor1_qual = calc_cast_qual(df_actors_films['Actor1_films'])\n",
    "actor2_film_scores, actor2_qual = calc_cast_qual(df_actors_films['Actor2_films'])\n",
    "actor3_film_scores, actor3_qual = calc_cast_qual(df_actors_films['Actor3_films'])\n",
    "actor4_film_scores, actor4_qual = calc_cast_qual(df_actors_films['Actor4_films'])\n",
    "actor5_film_scores, actor5_qual = calc_cast_qual(df_actors_films['Actor5_films'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_final = df_comb[['Title','Rel_year','RT_rev_cnt','RT_comb_score']]\n",
    "df_actors_final['Actor1'] = df_actors[['Actor1']]\n",
    "df_actors_final['Actor1_films'] = df_actors_films[['Actor1_films']]\n",
    "df_actors_final['Actor1_film_scores'] = actor1_film_scores\n",
    "df_actors_final['Actor1_qual']=actor1_qual\n",
    "\n",
    "df_actors_final['Actor2'] = df_actors[['Actor2']]\n",
    "df_actors_final['Actor2_films'] = df_actors_films[['Actor2_films']]\n",
    "df_actors_final['Actor2_film_scores'] = actor2_film_scores\n",
    "df_actors_final['Actor2_qual']=actor2_qual\n",
    "\n",
    "df_actors_final['Actor3'] = df_actors[['Actor3']]\n",
    "df_actors_final['Actor3_films'] = df_actors_films[['Actor3_films']]\n",
    "df_actors_final['Actor3_film_scores'] = actor3_film_scores\n",
    "df_actors_final['Actor3_qual']=actor3_qual\n",
    "\n",
    "df_actors_final['Actor4'] = df_actors[['Actor4']]\n",
    "df_actors_final['Actor4_films'] = df_actors_films[['Actor4_films']]\n",
    "df_actors_final['Actor4_film_scores'] = actor4_film_scores\n",
    "df_actors_final['Actor4_qual']=actor4_qual\n",
    "\n",
    "df_actors_final['Actor5'] = df_actors[['Actor5']]\n",
    "df_actors_final['Actor5_films'] = df_actors_films[['Actor5_films']]\n",
    "df_actors_final['Actor5_film_scores'] = actor5_film_scores\n",
    "df_actors_final['Actor5_qual']=actor5_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_final['Actor_avg_qual'] = df_actors_final[['Actor1_qual','Actor2_qual','Actor3_qual']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_actors_final2 = df_actors_final.sort_values(by='RT_comb_score', ascending=False)[['Title','RT_comb_score','Actor1','Actor1_films','Actor_avg_qual']]\n",
    "df_actors_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_final2.plot.scatter(x='Title', y='Actor_avg_qual',c='DarkBlue', rot=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_actors_final2.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_final.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part1 = pd.concat([df_final.iloc[:,0:7],df_dir_final.iloc[:,4:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part2 = pd.concat([df_part1,df_writer_final.iloc[:,4:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part3 = pd.concat([df_part2,df_actors_final.iloc[:,4:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_part3,df_comb.iloc[:,13:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_dir_lst = []\n",
    "for i in range(len(df_final)):\n",
    "    dirs = []\n",
    "    dir1 = df_final['Dir1'].iloc[i]\n",
    "    dir2 = df_final['Dir2'].iloc[i]\n",
    "    dir3 = df_final['Dir3'].iloc[i]\n",
    "    \n",
    "    dirs.append(dir1)\n",
    "    dirs.append(dir2)\n",
    "    dirs.append(dir3)\n",
    "    \n",
    "    writers = []\n",
    "    writer1 = df_final['Writer1'].iloc[i]\n",
    "    writer2 = df_final['Writer2'].iloc[i]\n",
    "    writer3 = df_final['Writer3'].iloc[i]\n",
    "    \n",
    "    writers.append(writer1)\n",
    "    writers.append(writer2)\n",
    "    writers.append(writer3)\n",
    "    \n",
    "    searching=True\n",
    "    \n",
    "    for d in dirs:\n",
    "        if d==None:\n",
    "            continue\n",
    "        elif d in writers:\n",
    "            writer_dir_lst.append(1)\n",
    "            searching=False\n",
    "            break\n",
    "        elif d!=dirs[-1]:\n",
    "            continue\n",
    "        else:\n",
    "            writer_dir_lst.append(0)\n",
    "            searching=False\n",
    "            break\n",
    "            \n",
    "    if searching==True:\n",
    "        writer_dir_lst.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.insert(loc=7, column='Writer_director', value=writer_dir_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.insert(loc=5, column='Plot_summ', value=plot_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.insert(loc=7, column='Rating_reason', value=mpaa_lst_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_pickle('master_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('master_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_scores_df2 = pd.concat([df_final[['Title','Rel_year']], movie_scores_df.iloc[:,2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmography_RT_data2 = filmography_RT_data[['Title','Year','Score','Revs','RT_comb_score','URL']]\n",
    "filmography_RT_data2 = filmography_RT_data2.rename(columns={\"Year\": \"Rel_year\", \"Score\": \"RT_score\", \"Revs\":'RT_review_cnt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_scores = pd.concat([movie_scores_df2, filmography_RT_data2], ignore_index=True)\n",
    "RT_scores = RT_scores.sort_values(by='Title', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_scores.to_csv('RT_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final IMDb pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr=0\n",
    "\n",
    "# get IMDb data\n",
    "plot_summaries = []\n",
    "mpaa_lst_final = []\n",
    "\n",
    "for url in list(df_final['IMDb_url']): #\n",
    "    \n",
    "    rating=''\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    \n",
    "    plot_info = soup.find('div',{'class':'sc-10602b09-2 jzJCdt'})\n",
    "    if 'sc-16ede01-2 gXUyNh' in str(plot_info):\n",
    "        plot_summary = plot_info.find('span',{'class':'sc-16ede01-2 gXUyNh'})\n",
    "        plot_summary = str(plot_summary).split('\"presentation\">')[1]\n",
    "        plot_summary = plot_summary.split('</span>')[0]\n",
    "        plot_summaries.append(plot_summary)\n",
    "    else:\n",
    "        plot_summaries.append('DNE')\n",
    "            \n",
    "    page = requests.get(url + 'parentalguide?ref_=tt_stry_pg')\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    certificate = soup.find('section',{'id':'certificates'})\n",
    "\n",
    "    mpaa_lst = []\n",
    "    mpaa = False\n",
    "    for row in certificate:\n",
    "        if 'MPAA' in str(row):\n",
    "            mpaa = str(row).split('<td>')[1]\n",
    "            mpaa = mpaa.split('</td>')[0]\n",
    "            reason = mpaa.split('for ')[1]\n",
    "            reason = re.split(', ', reason)\n",
    "            for r in reason:\n",
    "                r1 = re.split('and ', r)\n",
    "                for r2 in r1:\n",
    "                    if r2=='':\n",
    "                        continue\n",
    "                    else:\n",
    "                        r2 = r2.strip()\n",
    "                        r2 = r2.replace('.','')\n",
    "                        mpaa_lst.append(r2)\n",
    "                        mpaa=True\n",
    "\n",
    "    if mpaa==False:\n",
    "        mpaa_lst.append('No MPAA rating')\n",
    "\n",
    "    mpaa_lst_final.append(mpaa_lst)\n",
    "    \n",
    "    cntr+=1\n",
    "    print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_add_plot_kwords = pd.read_pickle('master_data.pkl')\n",
    "df_add_plot_kwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntr=0\n",
    "\n",
    "# get IMDb plot keywords\n",
    "plot_keywords = []\n",
    "\n",
    "for url in list(df_add_plot_kwords['IMDb_url']):\n",
    "    \n",
    "    page = requests.get(url + 'keywords?ref_=tt_stry_kw')\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    plot_section = soup.find('div',{'id':'keywords_content'})\n",
    "    \n",
    "    keyword_lst = []\n",
    "    if plot_section.find('div',{'id':'no_content'})!=None:\n",
    "        keyword_lst.append('DNE')\n",
    "    else:\n",
    "        table = plot_section.find('table')\n",
    "        table_row = table.find_all('tr')\n",
    "\n",
    "        for row in table_row:\n",
    "            keywords = row.find_all('td',{'class':'soda sodavote'})\n",
    "            for k in keywords:\n",
    "                keyword = k.get('data-item-keyword')\n",
    "                keyword_lst.append(keyword)\n",
    "    \n",
    "    plot_keywords.append(keyword_lst)\n",
    "    \n",
    "    cntr+=1\n",
    "    print(f'Row {cntr} completed. {round(((cntr/622)*100),0)}% done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keywords_copy = plot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [literal_eval(x) for x in list(df_add_plot_kwords['Genre'])]\n",
    "\n",
    "existing_material = []\n",
    "real_life = []\n",
    "\n",
    "for i in plot_keywords_copy:\n",
    "    \n",
    "    searching = True\n",
    "    if i==['DNE']:\n",
    "        searching = False\n",
    "        existing_material.append(np.nan)\n",
    "        real_life.append(np.nan)\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        l = []\n",
    "        for j in i:\n",
    "            if 'based on' in j:\n",
    "                searching = False\n",
    "                l.append(j)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if l!=[]:\n",
    "            for k in l:\n",
    "                if ('real' in k) or ('true' in k):\n",
    "                    real_life.append(1)\n",
    "                    break\n",
    "                elif k==l[-1]:\n",
    "                    real_life.append(0)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            for k in l:\n",
    "                if ('real' not in k) and ('true' not in k):\n",
    "                    existing_material.append(1)\n",
    "                    break\n",
    "                elif k==l[-1]:\n",
    "                    existing_material.append(0)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "    if searching==True:\n",
    "        existing_material.append(0)\n",
    "        real_life.append(0)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ind in zip(real_life,range(len(genres))):\n",
    "    if 'Documentary' in genres[ind]:\n",
    "        real_life[ind]=1\n",
    "        existing_material[ind]=0\n",
    "    elif 'Biography' in genres[ind]:\n",
    "        real_life[ind]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords = pd.DataFrame({'Plot_keywords': plot_keywords})\n",
    "df_keywords\n",
    "df_existing = pd.DataFrame(existing_material, columns=['Based_on_existing_material'])\n",
    "df_existing\n",
    "df_real = pd.DataFrame(real_life, columns=['Based_on_real_life'])\n",
    "df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a,b,c in zip(real_life,existing_material,range(len(plot_keywords_copy))):\n",
    "#     if (a==0) and (b==1):\n",
    "#         print(df_add_plot_kwords['Title'].iloc[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_lst = []\n",
    "for g in list(df_add_plot_kwords['Genre']):\n",
    "    g = literal_eval(g)\n",
    "    genre_lst.append(g)\n",
    "    \n",
    "df_add_plot_kwords['Genres']=genre_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres = pd.get_dummies(df_add_plot_kwords['Genres'].apply(pd.Series).stack(), prefix='Genre').sum(level=0)\n",
    "df_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_genre = ['Comedy','Documentary','Drama','Thriller']\n",
    "# weak_genres = ['Biography','Crime','Mystery']  # genres that are overpowered by others even if at the front\n",
    "# dominant_genres = ['Short','Documentary','Family','Horror','Comedy'] # genres that trump all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cat = []\n",
    "for r in list(df_add_plot_kwords['Rating']):\n",
    "    r=str(r)\n",
    "    if (r=='R') or (r=='TV-MA'):\n",
    "        rating_cat.append('Restricted')\n",
    "    elif (r=='TV-14') or (r=='PG-13'):\n",
    "        rating_cat.append('Limited')\n",
    "    elif r=='nan':\n",
    "        rating_cat.append('Not rated')\n",
    "    else:\n",
    "        rating_cat.append('General')\n",
    "\n",
    "df_ratings = pd.DataFrame(rating_cat, columns=['Rating_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.get_dummies(df_ratings, prefix=['Rating'], drop_first=True)\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb_final = pd.concat([df_add_plot_kwords.iloc[:,:5], df_genres, df_add_plot_kwords.iloc[:,5:6], \\\n",
    "                 df_keywords, df_existing, df_real, df_add_plot_kwords.iloc[:,6:7], df_ratings, \\\n",
    "                 df_add_plot_kwords.iloc[:,7:-1]], axis=1)\n",
    "df_comb_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb_final.to_pickle('master_data_v2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
